{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "import pprint\n",
    "import pickle\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import reverse_geocode\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import chisquare\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Root = '/Data/Promotion/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_one_line(filename, delimiter = '\\t', quote = csv.QUOTE_NONE):\n",
    "    '''a generator which produce one line of a given file'''\n",
    "    with open(filename, 'r') as file:\n",
    "        print('processing %s...' %(filename))\n",
    "        reader = csv.reader(file, delimiter=delimiter, quoting=quote)\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plat_cate = {\n",
    " 'blogs': 'blog',\n",
    " 'news': 'news_media',\n",
    " 'patent': 'knowledge_repo',\n",
    " 'f1000': 'knowledge_repo',\n",
    " 'peer_reviews': 'knowledge_repo',\n",
    " 'wikipedia': 'knowledge_repo',\n",
    " 'q&a': 'knowledge_repo',\n",
    " 'facebook': 'social_media',\n",
    " 'googleplus': 'social_media',\n",
    " 'linkedin': 'social_media',\n",
    " 'pinterest': 'social_media',\n",
    " 'reddit': 'social_media',\n",
    " 'twitter': 'social_media',\n",
    " 'video': 'social_media'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = set(['2013', '2014', '2015', '2016', '2017', '2018'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000000 lines...\n",
      "processed 2000000 lines...\n",
      "processed 3000000 lines...\n",
      "processed 4000000 lines...\n",
      "processed 5000000 lines...\n",
      "processed 6000000 lines...\n",
      "processed 7000000 lines...\n",
      "processed 8000000 lines...\n",
      "processed 9000000 lines...\n",
      "processed 10000000 lines...\n",
      "processed 11000000 lines...\n",
      "processed 12000000 lines...\n",
      "processed 13000000 lines...\n",
      "processed 14000000 lines...\n",
      "processed 15000000 lines...\n",
      "processed 16000000 lines...\n",
      "processed 17000000 lines...\n",
      "processed 18000000 lines...\n",
      "processed 19000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 21000000 lines...\n",
      "processed 22000000 lines...\n",
      "processed 23000000 lines...\n",
      "processed 24000000 lines...\n"
     ]
    }
   ],
   "source": [
    "paper_li = {}\n",
    "cn_lines = 0\n",
    "\n",
    "with open(Data_Root + 'merged.txt', 'r') as ofile:\n",
    "    for row in ofile:\n",
    "        cn_lines += 1\n",
    "        if cn_lines % 1000000 == 0:\n",
    "            print('processed %d lines...'%cn_lines)\n",
    "        if '}{\"altmetric_id\"' in row:\n",
    "            row = row.replace('}{\"altmetric_id\"', '}\\n{\"altmetric_id\"')\n",
    "        # this way works for both good and bad lines.\n",
    "        records = row.split('\\n')\n",
    "        for record in records:\n",
    "            if record != '':\n",
    "                paper = json.loads(record)\n",
    "                citation = paper['citation']\n",
    "                pubdate = ''\n",
    "                if 'pubdate' in citation:\n",
    "                    pubdate = citation['pubdate']\n",
    "                elif 'epubdate' in citation:\n",
    "                    pubdate = citation['epubdate']\n",
    "                dtype = ''\n",
    "                if 'type' in citation:\n",
    "                    dtype = citation['type']\n",
    "                doi = ''\n",
    "                if 'doi' in citation:\n",
    "                    doi = citation['doi']\n",
    "                # focus on research articles published in [2013, 2018]\n",
    "                if pubdate != '' and pubdate[:4] in years and dtype == 'article' and doi != '':\n",
    "                    # authors = []\n",
    "                    # if 'authors' in citation:\n",
    "                    #    authors = citation['authors']\n",
    "                    doi = doi.lower()\n",
    "                    jnl = ''\n",
    "                    if 'journal' in citation:\n",
    "                        jnl = citation['journal']\n",
    "                    scopus_subjects = []\n",
    "                    if 'scopus_subjects' in citation:\n",
    "                        scopus_subjects = citation['scopus_subjects']\n",
    "                    tweets = []\n",
    "                    if 'twitter' in paper['posts']:\n",
    "                        tweets = paper['posts']['twitter']\n",
    "                    # could add pub year later here.\n",
    "                    pp = {'journal': jnl, 'year': pubdate[:4], 'pub_date':pubdate, 'scopus_subjects': scopus_subjects, 'tweets': tweets}\n",
    "                    paper_li[doi] = pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6601528"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data_Root+'revision/papers_2013_2018.json', 'w') as ofile:\n",
    "    for doi in paper_li:\n",
    "        res = {'doi': doi, 'data': paper_li[doi]}\n",
    "        ofile.write(json.dumps(res)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_li = {}\n",
    "with open(Data_Root+'revision/papers_2013_2018.json', 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        res = json.loads(line)\n",
    "        paper_li[res['doi']] = res['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cn = defaultdict(int)\n",
    "\n",
    "for doi in paper_li:\n",
    "    paper = paper_li[doi]\n",
    "    year_cn[paper['year']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'2013': 875371,\n",
       "             '2016': 1187604,\n",
       "             '2014': 981578,\n",
       "             '2017': 1232949,\n",
       "             '2015': 1105317,\n",
       "             '2018': 1218709})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discs_broad = set(['Social Sciences', 'Life Sciences', 'Health Sciences', 'Physical Sciences', 'General'])\n",
    "\n",
    "# every paper has one of the five broad disciplines.\n",
    "# every paper has subfiels and their associated broad disciplines, so there is no need to map sub to broad separately.\n",
    "i = 0\n",
    "for doi in paper_li:\n",
    "    paper = paper_li[doi]\n",
    "    if len(paper['scopus_subjects']) > 0:\n",
    "        for sub in paper['scopus_subjects']:\n",
    "            if sub in discs_broad:\n",
    "                break\n",
    "        else:\n",
    "            if i < 10:\n",
    "                print(paper['scopus_subjects'])\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['General']\n",
    "# ['Physical Sciences', 'General', 'Mathematics', 'Engineering']\n",
    "sub_cn = defaultdict(int)\n",
    "for doi in paper_li:\n",
    "    paper = paper_li[doi]\n",
    "    for sub in paper['scopus_subjects']:\n",
    "        sub_cn[sub] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Health Sciences', 2176418),\n",
       " ('Medicine', 2059458),\n",
       " ('Life Sciences', 1536250),\n",
       " ('Physical Sciences', 1448583),\n",
       " ('Biochemistry, Genetics and Molecular Biology', 912785),\n",
       " ('Social Sciences', 659042),\n",
       " ('Agricultural and Biological Sciences', 478068),\n",
       " ('Physics and Astronomy', 419434),\n",
       " ('Chemistry', 394765),\n",
       " ('Engineering', 286386),\n",
       " ('Environmental Science', 258819),\n",
       " ('Materials Science', 242681),\n",
       " ('Earth and Planetary Sciences', 207029),\n",
       " ('Neuroscience', 204842),\n",
       " ('Immunology and Microbiology', 192735),\n",
       " ('Psychology', 182761),\n",
       " ('Pharmacology, Toxicology and Pharmaceutics', 172871),\n",
       " ('Chemical Engineering', 161673),\n",
       " ('Computer Science', 155894),\n",
       " ('Mathematics', 137190),\n",
       " ('Nursing', 119965),\n",
       " ('Business, Management and Accounting', 89325),\n",
       " ('Arts and Humanities', 87608),\n",
       " ('Health Professions', 79649),\n",
       " ('Economics, Econometrics and Finance', 73336),\n",
       " ('General', 62859),\n",
       " ('Energy', 61888),\n",
       " ('Veterinary', 45343),\n",
       " ('Decision Sciences', 34259),\n",
       " ('Dentistry', 26390)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num. of papers with a specific keyword\n",
    "sorted(sub_cn.items(), key=lambda x: x[1]*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social Sciecnes is created in the subfields because there is also a `social sciences` subfield category.\n",
    "discs = {'Life Sciences': 0, 'Health Sciences': 1, 'Physical Sciences': 2}\n",
    "\n",
    "# 26 subfields + 1 General\n",
    "fields = {'Social Sciences': 'Social Sciences',\n",
    " 'Materials Science': 'Physical Sciences',\n",
    " 'Engineering': 'Physical Sciences',\n",
    " 'Chemistry': 'Physical Sciences',\n",
    " 'Biochemistry, Genetics and Molecular Biology': 'Life Sciences',\n",
    " 'Medicine': 'Health Sciences',\n",
    " 'Nursing': 'Health Sciences',\n",
    " 'Agricultural and Biological Sciences': 'Life Sciences',\n",
    " 'Pharmacology, Toxicology and Pharmaceutics': 'Life Sciences',\n",
    " 'Neuroscience': 'Life Sciences',\n",
    " 'Business, Management and Accounting': 'Social Sciences',\n",
    " 'Economics, Econometrics and Finance': 'Social Sciences',\n",
    " 'Chemical Engineering': 'Physical Sciences',\n",
    " 'Physics and Astronomy': 'Physical Sciences',\n",
    " 'Computer Science': 'Physical Sciences',\n",
    " 'Decision Sciences': 'Social Sciences',\n",
    " 'Health Professions': 'Health Sciences',\n",
    " 'Psychology': 'Social Sciences',\n",
    " 'Immunology and Microbiology': 'Life Sciences',\n",
    " 'Dentistry': 'Health Sciences',\n",
    " 'Earth and Planetary Sciences': 'Physical Sciences',\n",
    " 'Environmental Science': 'Physical Sciences',\n",
    " 'Mathematics': 'Physical Sciences',\n",
    " 'Arts and Humanities': 'Social Sciences',\n",
    " 'Energy': 'Physical Sciences',\n",
    " 'Veterinary': 'Health Sciences',\n",
    " 'General': 'General'}\n",
    "\n",
    "field_ix = {fie: ix for ix, fie in enumerate(list(fields.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data_Root+'revision/alt_dois.pickle', 'wb') as ofile:\n",
    "    pickle.dump(list(paper_li.keys()), ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'journal': 'Criminology & Public Policy',\n",
       " 'scopus_subjects': [],\n",
       " 'tweets': [{'license': 'gnip',\n",
       "   'citation_ids': [46502633],\n",
       "   'author': {'tweeter_id': '3249180837'},\n",
       "   'tweet_id': '1029346936497823745'},\n",
       "  {'license': 'gnip',\n",
       "   'citation_ids': [46502633],\n",
       "   'author': {'tweeter_id': '3257735117'},\n",
       "   'tweet_id': '1032335832299040769'},\n",
       "  {'license': 'gnip',\n",
       "   'citation_ids': [46502633],\n",
       "   'author': {'tweeter_id': '987647622105194502'},\n",
       "   'tweet_id': '1032344553259524096'},\n",
       "  {'license': 'gnip',\n",
       "   'citation_ids': [46502633],\n",
       "   'author': {'tweeter_id': '19022052'},\n",
       "   'tweet_id': '1036613785598021632'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_li[doi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tid_name_sname_org = {}\n",
    "# tid_name_sname_retweet = {}\n",
    "\n",
    "# with open(Data_Root+'tweets_v2.json', 'r') as ifile:\n",
    "#     for line in ifile:\n",
    "#         line = json.loads(line)\n",
    "#         tid = line['id_str'] # line['created_at']\n",
    "#         if 'retweeted_status' in line:\n",
    "#             tid_name_sname_retweet[tid] = (line['user']['name'], line['user']['screen_name'])\n",
    "#         else:\n",
    "#             # pprint.pprint(line)\n",
    "#             tid_name_sname_org[tid] = (line['user']['name'], line['user']['screen_name'])\n",
    "#             # tid_date_all[tid] = datetime.strptime(tdate, '%a %b %d %H:%M:%S +0000 %Y')#.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "# with open(Data_Root+'tweets_v2_non_full_text.json', 'r') as ifile:\n",
    "#     for line in ifile:\n",
    "#         line = json.loads(line)\n",
    "#         tid = line['id_str']\n",
    "#         if 'retweeted_status' in line:\n",
    "#             tid_name_sname_retweet[tid] = (line['user']['name'], line['user']['screen_name'])\n",
    "#         else:\n",
    "#             tid_name_sname_org[tid] = (line['user']['name'], line['user']['screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85411606 tids in total, which contains 14412 tids that are not numeric string.\n",
    "# e.g., '1.5558470649316E+17', '1.5464891076012E+17', '1.6227338710274E+17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Fri Feb 17 09:01:16 +0000 2017',\n",
      " 'display_text_range': [0, 116],\n",
      " 'entities': {'hashtags': [],\n",
      "              'symbols': [],\n",
      "              'urls': [{'display_url': 'bit.ly/2kqg0ym',\n",
      "                        'expanded_url': 'http://bit.ly/2kqg0ym',\n",
      "                        'indices': [77, 100],\n",
      "                        'url': 'https://t.co/PcpdHjw86p'}],\n",
      "              'user_mentions': [{'id': 96741105,\n",
      "                                 'id_str': '96741105',\n",
      "                                 'indices': [0, 15],\n",
      "                                 'name': 'Newcastle University',\n",
      "                                 'screen_name': 'UniofNewcastle'},\n",
      "                                {'id': 96741105,\n",
      "                                 'id_str': '96741105',\n",
      "                                 'indices': [101, 116],\n",
      "                                 'name': 'Newcastle University',\n",
      "                                 'screen_name': 'UniofNewcastle'}]},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'full_text': '@UniofNewcastle r Bonnett Multiple racializations in a multiply '\n",
      "              'modern world https://t.co/PcpdHjw86p @UniofNewcastle',\n",
      " 'geo': None,\n",
      " 'id': 832515243918114816,\n",
      " 'id_str': '832515243918114816',\n",
      " 'in_reply_to_screen_name': 'UniofNewcastle',\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': 96741105,\n",
      " 'in_reply_to_user_id_str': '96741105',\n",
      " 'is_quote_status': False,\n",
      " 'lang': 'en',\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'source': '<a href=\"https://buffer.com\" rel=\"nofollow\">Buffer</a>',\n",
      " 'truncated': False,\n",
      " 'user': {'contributors_enabled': False,\n",
      "          'created_at': 'Wed Jan 08 11:03:35 +0000 2014',\n",
      "          'default_profile': False,\n",
      "          'default_profile_image': False,\n",
      "          'description': '2020 2-yr Impact Factor 2.755   ERS is the leading '\n",
      "                         'journal for the analysis of race and ethnicity '\n",
      "                         'throughout the world.\\n'\n",
      "                         'Instagram: ers.journal',\n",
      "          'entities': {'description': {'urls': []},\n",
      "                       'url': {'urls': [{'display_url': 'tandfonline.com/ERS',\n",
      "                                         'expanded_url': 'https://www.tandfonline.com/ERS',\n",
      "                                         'indices': [0, 23],\n",
      "                                         'url': 'https://t.co/bre8WUVtKW'}]}},\n",
      "          'favourites_count': 481,\n",
      "          'follow_request_sent': False,\n",
      "          'followers_count': 6430,\n",
      "          'following': False,\n",
      "          'friends_count': 2087,\n",
      "          'geo_enabled': False,\n",
      "          'has_extended_profile': False,\n",
      "          'id': 2281981230,\n",
      "          'id_str': '2281981230',\n",
      "          'is_translation_enabled': False,\n",
      "          'is_translator': False,\n",
      "          'lang': None,\n",
      "          'listed_count': 91,\n",
      "          'location': 'University of Surrey, UK',\n",
      "          'name': 'Ethnic and Racial Studies',\n",
      "          'notifications': False,\n",
      "          'profile_background_color': '000000',\n",
      "          'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "          'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "          'profile_background_tile': False,\n",
      "          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2281981230/1627554234',\n",
      "          'profile_image_url': 'http://pbs.twimg.com/profile_images/570507286116700160/Z-XpYfaO_normal.jpeg',\n",
      "          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/570507286116700160/Z-XpYfaO_normal.jpeg',\n",
      "          'profile_link_color': '1B95E0',\n",
      "          'profile_sidebar_border_color': '000000',\n",
      "          'profile_sidebar_fill_color': '000000',\n",
      "          'profile_text_color': '000000',\n",
      "          'profile_use_background_image': False,\n",
      "          'protected': False,\n",
      "          'screen_name': 'ERSjournal',\n",
      "          'statuses_count': 6691,\n",
      "          'time_zone': None,\n",
      "          'translator_type': 'none',\n",
      "          'url': 'https://t.co/bre8WUVtKW',\n",
      "          'utc_offset': None,\n",
      "          'verified': False,\n",
      "          'withheld_in_countries': []}}\n"
     ]
    }
   ],
   "source": [
    "# An example tweet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data_Root+'tid_name_sname.json', 'w') as ofile:\n",
    "    for tid in tid_name_sname_org:\n",
    "        res = {'tid': tid, 'type': 'org', 'data': tid_name_sname_org[tid]}\n",
    "        ofile.write(json.dumps(res)+'\\n')\n",
    "    for tid in tid_name_sname_retweet:\n",
    "        res = {'tid': tid, 'type': 're', 'data': tid_name_sname_retweet[tid]}\n",
    "        ofile.write(json.dumps(res)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid_name_sname_org = {}\n",
    "tid_name_sname_retweet = {}\n",
    "\n",
    "with open(Data_Root+'tid_name_sname.json', 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        res = json.loads(line)\n",
    "        if res['type'] == 'org':\n",
    "            tid_name_sname_org[res['tid']] = res['data']\n",
    "        else:\n",
    "            tid_name_sname_retweet[res['tid']] = res['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34362660"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tid_name_sname_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43134644"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tid_name_sname_retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_author_list = defaultdict(list)\n",
    "\n",
    "with open(Data_Root+'revision/dois_authors_mag.json', 'r') as ofile:\n",
    "    for row in ofile:\n",
    "        row = json.loads(row)\n",
    "        doi, authors = row['doi'], row['authors']\n",
    "        uniq_authors = defaultdict(lambda: ['', [], ''])\n",
    "        for aid, affi_id, seq, name in authors:\n",
    "            seq = int(seq)\n",
    "            uniq_authors[seq][0] = aid\n",
    "            uniq_authors[seq][1].append(affi_id)\n",
    "            # normalized author name.\n",
    "            uniq_authors[seq][2] = unidecode.unidecode(name).strip()\n",
    "        # could be [1, 2, 4, 6]; [2, 3]\n",
    "        uniq_seqs = sorted(uniq_authors)\n",
    "        # must be: 1, 2, ..., max\n",
    "        if uniq_seqs == list(range(1, len(uniq_seqs)+1)):\n",
    "            last_seq = len(uniq_seqs)\n",
    "            if last_seq == 1:\n",
    "                aid, affi_ids, name = uniq_authors[last_seq]\n",
    "                affis = '|'.join(affi_ids)\n",
    "                doi_author_list[doi].append([aid, affis, last_seq, name, 'solo_author'])\n",
    "            else:\n",
    "                for seq in uniq_authors:\n",
    "                    aid, affi_ids, name = uniq_authors[seq]\n",
    "                    affis = '|'.join(affi_ids)\n",
    "                    if seq == 1:\n",
    "                        doi_author_list[doi].append([aid, affis, seq, name, 'first_position'])\n",
    "                    elif seq == last_seq:\n",
    "                        doi_author_list[doi].append([aid, affis, seq, name, 'last_position'])\n",
    "                    else:\n",
    "                        doi_author_list[doi].append([aid, affis, seq, name, 'middle_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6061830"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doi_author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2899491243', '', 1, 'Annie J. Tsay', 'solo_author']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_author_list['10.1001/amajethics.2018.1003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_names = set()\n",
    "for doi in doi_author_list:\n",
    "    for aut in doi_author_list[doi]:\n",
    "        author_name = aut[3]\n",
    "        author_names.add(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8507860"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data_Root+'revision/author_names.txt', 'w') as ofile:\n",
    "    for author_name in author_names:\n",
    "        ofile.write(author_name+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict gender based on name using Ford et al.\n",
    "# see `1_3_predict_gender.ipynb` (run code using Python 2.7)\n",
    "\n",
    "name_gender = {}\n",
    "with open(Data_Root+'revision/name_gender.txt', 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        # get rid of '\\n' at the end of each line\n",
    "        line = line.strip()\n",
    "        name, gender = line.split('||')\n",
    "        name_gender[name] = gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8454095"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see `1_2_mag.ipynb`\n",
    "\n",
    "aid_metric = {}\n",
    "\n",
    "with open(Data_Root+'revision/aids_metric_mag.json', 'r') as ofile:\n",
    "    for row in ofile:\n",
    "        row = json.loads(row)\n",
    "        aid, metric = row['aid'], row['metric']\n",
    "        aid_metric[aid] = [int(num) for num in metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9777945"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aid_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_year_paper_count = {}\n",
    "with open(Data_Root+'revision/aid_year_paper_list.json', 'r') as ofile:\n",
    "    for row in ofile:\n",
    "        row = json.loads(row)\n",
    "        aid = row['aid']\n",
    "        aid_year_paper_count[aid] = {year: len(pids) for year, pids in row['year_pids'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9776277"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aid_year_paper_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2015': 7,\n",
       " '2010': 2,\n",
       " '2013': 1,\n",
       " '2011': 1,\n",
       " '2014': 4,\n",
       " '2008': 1,\n",
       " '2005': 2,\n",
       " '2003': 1,\n",
       " '2012': 3,\n",
       " '2016': 8,\n",
       " '2017': 7,\n",
       " '2018': 16}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aid_year_paper_count['2704166667']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see `1_2_mag.ipynb`\n",
    "\n",
    "aid_cites_upto_years = {}\n",
    "\n",
    "with open(Data_Root+'revision/aid_cites_upto_years.json', 'r') as ofile:\n",
    "    for row in ofile:\n",
    "        row = json.loads(row)\n",
    "        aid, cites = row['aid'], row['cum_cites']\n",
    "        aid_cites_upto_years[aid] = cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8744877"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aid_cites_upto_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2015': 285, '2016': 415, '2017': 598, '2012': 120, '2013': 146, '2014': 200}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aid_cites_upto_years['2704166667']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_name(name):\n",
    "    words = name.split()\n",
    "    if len(words) <= 1:\n",
    "        payload = 'invalid'\n",
    "    else:\n",
    "        payload = {'Fname': words[0], 'Lname': words[-1]}\n",
    "    return payload\n",
    "\n",
    "def check_match_author(tname_tuple, name_tupe):\n",
    "    hname, sname = tname_tuple\n",
    "    hname = unidecode.unidecode(hname).lower()\n",
    "    sname = unidecode.unidecode(sname).lower()\n",
    "    htokens, stokens = [hname], [sname]\n",
    "    if ' ' in hname:\n",
    "        htokens = hname.split(' ')\n",
    "    elif '_' in hname:\n",
    "        htokens = hname.split('_')\n",
    "    if ' ' in sname:\n",
    "        stokens = sname.split(' ')\n",
    "    elif '_' in sname:\n",
    "        stokens = sname.split('_')\n",
    "    tokens = htokens + stokens\n",
    "    fname, lname = name_tupe['Fname'], name_tupe['Lname']\n",
    "    fname = fname.lower()\n",
    "    lname = lname.lower()\n",
    "    if len(tokens) > 2:\n",
    "        # don't use first name if it is initial. e.g., True 3 M. B. Mendonca (Plecoforum.nl)\n",
    "        if (len(fname) == 1) or (len(fname) == 2 and fname[-1] == '.'):\n",
    "            if lname in tokens:\n",
    "                return True\n",
    "        else:\n",
    "            if (fname in tokens) or (lname in tokens):\n",
    "                return True\n",
    "        return False\n",
    "    # both user name and screen name are single-token string: use containment matching only for names with >= 4 chars.\n",
    "    else:\n",
    "        if len(fname) >= 4:\n",
    "            if (fname in hname) or (fname in sname):\n",
    "                return True\n",
    "        if len(lname) >= 4:\n",
    "            if (lname in hname) or (lname in sname):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans(\",'&:-\", \"     \")\n",
    "\n",
    "def norm_string(aff_name):\n",
    "    aff_name = aff_name.lower().translate(table)\n",
    "    aff_name = ' '.join(aff_name.split())\n",
    "    return aff_name\n",
    "\n",
    "def get_journal_rank(journal):\n",
    "    if journal == 'unknown':\n",
    "        return np.NaN\n",
    "    else:\n",
    "        nname = norm_string(journal)\n",
    "        if nname in journal_impact:\n",
    "            return journal_impact[nname]\n",
    "        else:\n",
    "            return np.NaN\n",
    "        \n",
    "def get_journal_cate(journal):\n",
    "    if journal in top_journals or journal == 'unknown':\n",
    "        return journal\n",
    "    else:\n",
    "        return 'lose journal'\n",
    "\n",
    "def get_affi_name(affis):\n",
    "    affi_li = affis.split('|')\n",
    "    univs = []\n",
    "    for affi_id in affi_li:\n",
    "        if affi_id in affi_name:\n",
    "            univs.append(affi_name[affi_id])\n",
    "    if len(univs) == 0:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return '|'.join(univs)\n",
    "    \n",
    "def get_affi_cate(affis):\n",
    "    affi_li = affis.split('|')\n",
    "    cate_li = set()\n",
    "    for affi_id in affi_li:\n",
    "        # many affi ids in MAG do not have latitude and longitude info for us to infer their country\n",
    "        if affi_id in affi_country:\n",
    "            country = affi_country[affi_id]\n",
    "            if country == 'United States':\n",
    "                cate_li.add('domestic')\n",
    "            else:\n",
    "                cate_li.add('international')\n",
    "        else:\n",
    "            cate_li.add('unknown')\n",
    "    if 'domestic' in cate_li:\n",
    "        return 'domestic'\n",
    "    elif 'international' in cate_li:\n",
    "        return 'international'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "        \n",
    "def get_affi_rank(affis):\n",
    "    affi_li = affis.split('|')\n",
    "    rank_li = []\n",
    "    for affi_id in affi_li:\n",
    "        if affi_id in affi_rank:\n",
    "            rank_li.append(affi_rank[affi_id])\n",
    "    if len(rank_li) == 0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return min(rank_li)\n",
    "\n",
    "def get_author_rank(aid):\n",
    "    if aid in aid_metric:\n",
    "        return aid_metric[aid][0]\n",
    "    else:\n",
    "        return np.NaN    \n",
    "\n",
    "def get_aut_pub_cn(row):\n",
    "    pub_year, aid = row['pub_year'], row['author_id']\n",
    "    if aid in aid_year_paper_count:\n",
    "        total = 0\n",
    "        pub_year = int(pub_year)\n",
    "        for year, cn in aid_year_paper_count[aid].items():\n",
    "            if int(year) <= pub_year:\n",
    "                total += cn\n",
    "        return total\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "def get_aut_cite_cn(row):\n",
    "    pub_year, aid = row['pub_year'], row['author_id']\n",
    "    if aid in aid_cites_upto_years:\n",
    "        ykey = str(int(pub_year) - 1)\n",
    "        if ykey in aid_cites_upto_years[aid]:\n",
    "            return aid_cites_upto_years[aid][ykey]\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "def get_field_vector(doi):\n",
    "    vec = [0] * len(fields)\n",
    "    for field in paper_li[doi]['scopus_subjects']:\n",
    "        if field in field_ix:\n",
    "            vec[field_ix[field]] = 1\n",
    "    return vec\n",
    "\n",
    "def get_disc_vector(doi):\n",
    "    vec = [0] * len(discs)\n",
    "    for field in paper_li[doi]['scopus_subjects']:\n",
    "        if field in discs:\n",
    "            vec[discs[field]] = 1\n",
    "    return vec\n",
    "\n",
    "def clean_field_name(name):\n",
    "    ans = ''\n",
    "    for ch in name:\n",
    "        if ch in ['(', ',', ')', '-', ' ', \"'\"]:\n",
    "            ans += '_'\n",
    "        else:\n",
    "            ans += ch\n",
    "    return ans\n",
    "\n",
    "# make `tid_name_sname` a param.\n",
    "def get_self_promo(row, tid_name_sname):\n",
    "    doi, author_name = row['doi'], row['author_name']\n",
    "    paper = paper_li[doi]\n",
    "    mtids = [tweet['tweet_id'] for tweet in paper['tweets'] if tweet['tweet_id'] in tid_name_sname]\n",
    "    # must already be a dict.\n",
    "    name_tupe = get_author_name(author_name)\n",
    "    self_promo = False\n",
    "    mat_tid = ''\n",
    "    matches = []\n",
    "    for tid in mtids:\n",
    "        tname_tupe = tid_name_sname[tid]\n",
    "        if check_match_author(tname_tupe, name_tupe):\n",
    "            matches.append((tid, tid_name_sname[tid][0]))\n",
    "    if len(matches) >= 1:\n",
    "        self_promo = True\n",
    "        if len(matches) == 1:\n",
    "            mat_tid = matches[0][0]\n",
    "        else:\n",
    "            tid_scores = {}\n",
    "            for tid, tw_name in matches:\n",
    "                tid_scores[tid] = fuzz.ratio(tw_name, author_name)\n",
    "            mat_tid = sorted(tid_scores.keys(), key=lambda tid: tid_scores[tid], reverse=True)[0]\n",
    "    return (self_promo, mat_tid)\n",
    "\n",
    "def get_self_promo_comb_tid(row):\n",
    "    p_0, p_1, p_2 = row['self_promotion'], row['self_promotion_original'], row['self_promotion_retweet']\n",
    "    if p_0:\n",
    "        if p_1:\n",
    "            return row['matched_tid_original']\n",
    "        # p_2 must be true\n",
    "        else:\n",
    "            return row['matched_tid_retweet']\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100000 papers...\n",
      "processed 200000 papers...\n",
      "processed 300000 papers...\n",
      "processed 400000 papers...\n",
      "processed 500000 papers...\n",
      "processed 600000 papers...\n",
      "processed 700000 papers...\n",
      "processed 800000 papers...\n",
      "processed 900000 papers...\n",
      "processed 1000000 papers...\n",
      "processed 1100000 papers...\n",
      "processed 1200000 papers...\n",
      "processed 1300000 papers...\n",
      "processed 1400000 papers...\n",
      "processed 1500000 papers...\n",
      "processed 1600000 papers...\n",
      "processed 1700000 papers...\n",
      "processed 1800000 papers...\n",
      "processed 1900000 papers...\n",
      "processed 2000000 papers...\n",
      "processed 2100000 papers...\n",
      "processed 2200000 papers...\n",
      "processed 2300000 papers...\n",
      "processed 2400000 papers...\n",
      "processed 2500000 papers...\n",
      "processed 2600000 papers...\n",
      "processed 2700000 papers...\n",
      "processed 2800000 papers...\n",
      "processed 2900000 papers...\n",
      "processed 3000000 papers...\n",
      "processed 3100000 papers...\n",
      "processed 3200000 papers...\n",
      "processed 3300000 papers...\n",
      "processed 3400000 papers...\n",
      "processed 3500000 papers...\n",
      "processed 3600000 papers...\n",
      "processed 3700000 papers...\n",
      "processed 3800000 papers...\n",
      "processed 3900000 papers...\n",
      "processed 4000000 papers...\n",
      "processed 4100000 papers...\n",
      "processed 4200000 papers...\n",
      "processed 4300000 papers...\n",
      "processed 4400000 papers...\n",
      "processed 4500000 papers...\n",
      "processed 4600000 papers...\n",
      "processed 4700000 papers...\n",
      "processed 4800000 papers...\n",
      "processed 4900000 papers...\n",
      "processed 5000000 papers...\n",
      "processed 5100000 papers...\n",
      "processed 5200000 papers...\n",
      "processed 5300000 papers...\n",
      "processed 5400000 papers...\n",
      "processed 5500000 papers...\n",
      "processed 5600000 papers...\n",
      "processed 5700000 papers...\n",
      "processed 5800000 papers...\n",
      "processed 5900000 papers...\n",
      "processed 6000000 papers...\n",
      "processed 6100000 papers...\n",
      "processed 6200000 papers...\n",
      "processed 6300000 papers...\n",
      "processed 6400000 papers...\n",
      "processed 6500000 papers...\n",
      "processed 6600000 papers...\n"
     ]
    }
   ],
   "source": [
    "reg_data = []\n",
    "\n",
    "cn = 0\n",
    "for doi in paper_li:\n",
    "    cn += 1\n",
    "    if cn%100000 == 0:\n",
    "        print('processed %d papers...'%cn)\n",
    "    pub_year = paper_li[doi]['year']\n",
    "    if doi in doi_author_list:\n",
    "        # get pub year from paper_li later\n",
    "        for author in doi_author_list[doi]:\n",
    "            aid, affis, seq, author_name, pos = author\n",
    "            name_tupe = get_author_name(author_name)\n",
    "            if type(name_tupe) == dict:\n",
    "#                 self_promo, mat = get_self_promo(doi, author_name)\n",
    "                reg_data.append([doi, pub_year, author_name, seq, pos, aid, affis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = pd.DataFrame(reg_data, columns = ['doi', 'pub_year', 'author_name', 'authorship_seq', 'authorship_pos', \\\n",
    "                                             'author_id', 'affiliation_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['DV'] = reg_data.apply(lambda row: get_self_promo(row, tid_name_sname_org), axis = 1)\n",
    "reg_data[['self_promotion_original', 'matched_tid_original']] = pd.DataFrame(reg_data.DV.values.tolist(), index = reg_data.index)\n",
    "reg_data = reg_data.drop(columns=['DV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740636"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reg_data['self_promotion_original'] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a different `tid_name_sname` for retweets.\n",
    "reg_data['DV'] = reg_data.apply(lambda row: get_self_promo(row, tid_name_sname_retweet), axis = 1)\n",
    "reg_data[['self_promotion_retweet', 'matched_tid_retweet']] = pd.DataFrame(reg_data.DV.values.tolist(), index = reg_data.index)\n",
    "reg_data = reg_data.drop(columns=['DV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515462"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reg_data['self_promotion_retweet'] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['self_promotion'] = (reg_data['self_promotion_retweet'] == True) | (reg_data['self_promotion_original'] == True)\n",
    "reg_data['matched_tid'] = reg_data.apply(lambda row: get_self_promo_comb_tid(row), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061401"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reg_data['self_promotion'] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Altmetric data: JAMA: Journal of the American Medical Association\n",
    "# WOS data: JAMA-JOURNAL OF THE AMERICAN MEDICAL ASSOCIATION\n",
    "# 2018 version.\n",
    "journal_impact = {}\n",
    "for line in yield_one_line(Data_Root+'wos_jcr.csv', delimiter=',', quote=csv.QUOTE_ALL):\n",
    "    rank, title, cites, jif, eigen = line\n",
    "    try:\n",
    "        impact = float(jif)\n",
    "        ntitle = norm_string(title)\n",
    "        journal_impact[ntitle] = impact\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11873"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(journal_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "affi_rank = {}\n",
    "affi_country = {}\n",
    "affi_name = {}\n",
    "for line in yield_one_line(Data_Root+'Affiliations.txt'):\n",
    "    affi_id, rank, dname, lat, lon = line[0], line[1], line[3], line[9], line[10]\n",
    "    affi_rank[affi_id] = int(rank)\n",
    "    affi_name[affi_id] = dname\n",
    "    if lat != \"\" and lon != \"\":\n",
    "        lat, lon = float(lat), float(lon)\n",
    "        res = reverse_geocode.search([(lat, lon)])\n",
    "        country = res[0]['country']\n",
    "        affi_country[affi_id] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affi_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['gender'] = reg_data['author_name'].apply(lambda author_name: name_gender[author_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['num_authors'] = reg_data['doi'].apply(lambda doi: len(doi_author_list[doi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['author_pub_count'] = reg_data.apply(lambda row: get_aut_pub_cn(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_data['author_citation'] = reg_data.apply(lambda row: get_aut_cite_cn(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_data['author_feats'] = reg_data['author_id'].map(aid_metric)\n",
    "# reg_data[['author_rank', 'num_paper', 'num_cite']] = pd.DataFrame(reg_data['author_feats'].values.tolist(), index = reg_data.index)\n",
    "\n",
    "reg_data['author_rank'] = reg_data['author_id'].apply(get_author_rank)\n",
    "reg_data['affiliation_name'] = reg_data['affiliation_ids'].apply(get_affi_name)\n",
    "reg_data['affiliation_cate'] = reg_data['affiliation_ids'].apply(get_affi_cate)\n",
    "reg_data['affiliation_rank'] = reg_data['affiliation_ids'].apply(get_affi_rank)\n",
    "reg_data['journal_title'] = reg_data['doi'].apply(lambda doi: paper_li[doi]['journal'] if paper_li[doi]['journal'] != '' else 'unknown')\n",
    "reg_data['journal_impact'] = reg_data['journal_title'].apply(get_journal_rank)\n",
    "top_journals = set([jname for jname, num in Counter(reg_data['journal_title']).most_common()[:100] if jname != 'unknown'])\n",
    "reg_data['top_journal'] = reg_data['journal_title'].apply(get_journal_cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_rate_pub_cn = {}\n",
    "for aid, sub in reg_data.groupby('author_id'):\n",
    "    pub_cn = len(sub)\n",
    "    aid_rate_pub_cn[aid] = (pub_cn, np.mean(sub['self_promotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['pub_cn_rate'] = reg_data['author_id'].map(aid_rate_pub_cn)\n",
    "reg_data[['author_num_papers_in_data', 'author_self_promotion_rate']] = pd.DataFrame(reg_data.pub_cn_rate.values.tolist(), index = reg_data.index)\n",
    "reg_data = reg_data.drop(columns=['pub_cn_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aid_rate_pub_cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['field_vector'] = reg_data['doi'].apply(get_field_vector)\n",
    "reg_data[list(fields.keys())] = pd.DataFrame(reg_data.field_vector.values.tolist(), index = reg_data.index)\n",
    "reg_data = reg_data.drop(columns=['field_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name_clean_map = {name: clean_field_name(name) for name in fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Social_Sciences + Materials_Science + Engineering + Chemistry + Biochemistry__Genetics_and_Molecular_Biology + Medicine + Nursing + Agricultural_and_Biological_Sciences + Pharmacology__Toxicology_and_Pharmaceutics + Neuroscience + Business__Management_and_Accounting + Economics__Econometrics_and_Finance + Chemical_Engineering + Physics_and_Astronomy + Computer_Science + Decision_Sciences + Health_Professions + Psychology + Immunology_and_Microbiology + Dentistry + Earth_and_Planetary_Sciences + Environmental_Science + Mathematics + Arts_and_Humanities + Energy + Veterinary + General'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' + '.join(field_name_clean_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = reg_data.rename(columns=field_name_clean_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['disc_vector'] = reg_data['doi'].apply(get_disc_vector)\n",
    "reg_data[list(discs.keys())] = pd.DataFrame(reg_data.disc_vector.values.tolist(), index = reg_data.index)\n",
    "reg_data = reg_data.drop(columns=['disc_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_name_clean_map = {name: clean_field_name(name) for name in discs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = reg_data.rename(columns=disc_name_clean_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.132341e+07\n",
       "mean     7.565661e+03\n",
       "std      1.749812e+03\n",
       "min      4.446000e+03\n",
       "25%      6.308000e+03\n",
       "50%      7.255000e+03\n",
       "75%      8.531000e+03\n",
       "max      1.923800e+04\n",
       "Name: affiliation_rank, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.affiliation_rank.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10632779"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reg_data.affiliation_rank.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['affiliation_rank_cate'] = pd.qcut(reg_data['affiliation_rank'], q=10, labels=False, duplicates='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    2156287\n",
       "2.0    2142618\n",
       "1.0    2142223\n",
       "0.0    2139495\n",
       "8.0    2133045\n",
       "9.0    2130291\n",
       "7.0    2122499\n",
       "4.0    2121821\n",
       "6.0    2117948\n",
       "3.0    2117179\n",
       "Name: affiliation_rank_cate, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.affiliation_rank_cate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21323406"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN values are ignored (no cate assigned)\n",
    "np.sum(reg_data.affiliation_rank_cate.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group num. of pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.195454e+07\n",
       "mean     8.909679e+01\n",
       "std      1.919776e+02\n",
       "min      0.000000e+00\n",
       "25%      4.000000e+00\n",
       "50%      2.300000e+01\n",
       "75%      8.900000e+01\n",
       "max      4.871000e+03\n",
       "Name: author_pub_count, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.author_pub_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They added an option duplicates='raise'|'drop' to control whether to raise on duplicated edges or to drop them, which would result in less bins than specified, and some larger (with more elements) than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, bins = pd.qcut(reg_data['author_pub_count'], q=10, labels=False, duplicates='drop', retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    1.,    3.,    6.,   13.,   23.,   41.,   69.,  118.,\n",
       "        229., 4871.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data['author_pub_count_cate'] = pd.qcut(reg_data['author_pub_count'], q=10, labels=False, duplicates='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4064583\n",
       "3.0    3427521\n",
       "5.0    3328370\n",
       "9.0    3194847\n",
       "8.0    3169567\n",
       "6.0    3147448\n",
       "7.0    3135451\n",
       "1.0    3013007\n",
       "4.0    2951678\n",
       "2.0    2522065\n",
       "Name: author_pub_count_cate, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.author_pub_count_cate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31956185, 58)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data.to_csv(Data_Root+\"revision/reg_data.csv\", index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utype = {'author_id': str, 'matched_tid': 'str', 'matched_tid_original': 'str', 'matched_tid_retweet': 'str'}\n",
    "reg_data = pd.read_csv(Data_Root+\"revision/reg_data.csv\", header=0, dtype=utype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
